# Data-Analytics-Intership-At-Prepinsta-

**Following Are The Projects That I Have Worked During The Intership** 

**Project 1 :**

**Bike Buyers Dashboard in Google Sheets: 
Explore the behavior and preferences of bike buyers through a comprehensive dashboard created in Google Sheets. Analyze demographics, purchasing trends, and derive meaningful insights for business decision-making.**

**DashBoard: https://docs.google.com/spreadsheets/d/10YLlEYCo8b7DbklXzD6cDAlP_l20n_FdFSRGCnul-30/edit?usp=sharing**

**Documentation: https://drive.google.com/file/d/1p_dq2Zp8ZJZO2CUdc_wR5mJJebPNa3ma/view?usp=sharing**

**Google Sheet :https://docs.google.com/spreadsheets/d/10YLlEYCo8b7DbklXzD6cDAlP_l20n_FdFSRGCnul-30/edit?usp=sharing**



**Project 2 :**

**Frog Leap Puzzle Game :
Developed a captivating puzzle game where players navigate a frog across a grid-like environment, aiming to leap from the left to the right edge strategically. The game offers an engaging challenge by allowing players to control frog movement using keyboard inputs. The objective is to solve the puzzle by moving the frogs while adhering to specific rules and constraints. Implemented on Google Colab, the game displays a visual grid with green and brown frogs, creating an immersive experience for players to enjoy. Explore multiple levels and potential enhancements to elevate the gameplay and make it more intriguing.**

**Google Colab Link : https://colab.research.google.com/drive/1Lr2E4xH0mnN90nN3GTxyUkAEtXovkyx8?usp=sharing**

**Documentation : https://drive.google.com/file/d/1eNyO9kqUC-D9L-n5opyXvqxLLxtWDExm/view?usp=sharing**



**Project 3 :**

**1 : Data Cleaning (Pandas) :**

**This project aims to refine and optimize a dataset using the powerful Pandas library in Python for data cleaning and manipulation. Here's an overview of the tasks involved:
Missing Values Handling: It investigates if any data is missing and suggests methods to handle these gaps for each indicator, ensuring a complete dataset.
Data Types Verification: Analyzing the data types of each indicator ensures they align with their expected types, such as numerical or categorical, fostering accurate analysis.
Outlier Identification: Detecting potential outliers in numerical indicators (like Age, Salary, Rating) and deciding whether to adjust or remove them to maintain data integrity.
Salary Formatting: Examining the structure of the Salary column to standardize its format for consistent analysis.
Location Standardization: Checking consistency in location entries and proposing methods to standardize them for uniformity.
Established Column Examination: Investigating the Established column for inconsistencies or anomalies that could impact analysis.
Easy Apply Analysis: Assessing the Easy Apply column for boolean values or necessitating transformations for better analysis.
Rating Range Investigation: Examining the range of values in the Rating column, checking if they align with expected rating scales, and managing outliers.
Age Distribution Check: Analyzing the distribution of values in the Age column to identify unusual entries that might affect analysis.
Handling Special Characters: Scrutinizing text-based columns (e.g., Location) for special characters or inconsistencies requiring cleaning.
Data Integrity Assurance: Ensuring data integrity by cross-referencing entries, such as verifying alignment between the Established and Age columns.
Easy Apply Transformation: Transforming the Easy Apply column if it contains non-boolean values for better usability.
Location Accuracy Assessment: Evaluating the accuracy of location entries and rectifying misspelled or ambiguous locations for clarity.
Categorical Data Handling: Encoding or transforming categorical indicators into a suitable format for analysis.
Consistent Rating Scale: Normalizing or adjusting the Rating column for a consistent rating scale conducive to uniform analysis.
These meticulous steps focus on refining the dataset, rectifying inconsistencies, and ensuring the reliability and accuracy of the data, ultimately aiming to reveal meaningful patterns and insights for comprehensive analysis.**

**Google Colab Link : https://colab.research.google.com/drive/1djBhvkIa3Jfno-OkevkTqv_E32JeHAkU?usp=sharing**

**Documentation : https://drive.google.com/file/d/12GBBrFO8o7dCNOJb66WDcpR7X2VoZDDD/view?usp=sharing**


**2 : Data Cleaning (chipotle.tsv) :**

**This project involves utilizing the Pandas library in Python to perform comprehensive data cleaning and preparation tasks on the Chipotle dataset, which is in TSV (Tab-Separated Values) format. Here's an overview of the tasks:
Reading TSV File: The dataset is read into a Pandas DataFrame using pd.read_csv with sep='\t' to handle the TSV format.
Missing Values Handling: Each column (Order ID, Quantity, Item Name, Choice Description, Item Price) is examined for missing values, and strategies for handling these gaps in the data are proposed for robust analysis.
Data Types Verification: Data types for each column are checked to ensure they align with their expected types. Adjustments are suggested if needed for accurate analysis.
Duplicated Entries Management: Identification and handling of duplicated entries within the dataset. The impact of duplicates on analysis is discussed, and appropriate actions are recommended.
Quantity and Item Price Examination: An investigation of the Quantity and Item Price columns is conducted to identify inconsistencies or anomalies requiring correction for data accuracy.
Choice Description Analysis: Evaluation of the Choice Description column, especially when multiple descriptions exist for a single item, to determine suitable handling methods.
Handling Special Characters: Detection and resolution of special characters present in text-based columns (e.g., Item Name, Choice Description) to ensure consistency in data representation.
Order ID Integrity Check: Validation of Order ID column for irregularities or patterns that need verification to maintain data integrity.
Item Name Standardization: Standardizing the Item Name column by unifying variations for better analysis and consistent representation.
Quantity and Price Relationships Investigation: Analyzing the relationships between Quantity and Item Price to identify cases requirin adjustments for accurate analysis.
Data Integrity Verification: Ensuring data integrity by validating that quantities and prices align accurately with the correspondingitems and descriptions.
Converting to CSV (Optional): Converting the cleaned dataset to a CSV file using to_csv function with sep=',' if necessary
Handling Categorical Data: Encoding or transforming categorical columns (e.g., Item Name) into a suitable format for compehensive analysis.
Consistent Quantity and Price Units: Ensuring consistency in units for Quantity and Item Price, and suggesting conversios or adjustments for uniform analysis if needed.
These meticulous steps aim to refine the Chipotle dataset, rectify inconsistencies, handle missing values, validate dat integrity, and prepare it for further analysis, thereby enhancing the reliability and accuracy of insights derived from the data.**

**Google Colab Link : https://colab.research.google.com/drive/1h5j58QVVsWPHIkJAFDQN5kQLo_tYNZmm?usp=sharing**

**Documentation : https://drive.google.com/file/d/1J_zNgt5pmUn6aVTp9mAYxATW6_rK_xbi/view?usp=sharing**


**Project 4 :**

**World Bank Data :**

**This repository recreates Hans Rosling's captivating TED talk visualization using Python's powerful data science tools. Dive into the intricate tapestry of global health trends, dissect population dynamics, and unveil the intricate correlations between life expectancy and fertility rates. The project encompasses a mesmerizing animated graph, bringing to life the nuanced shifts from 1960 to 2016. Supplementing the animated spectacle are static visualizations offering nuanced perspectives on historical and regional health patterns. Simply follow the provided steps for seamless data preprocessing, merging, and visualization, unlocking a comprehensive understanding of this iconic representation of global health data.**

**Google Colab Link : https://colab.research.google.com/drive/1kE1mcDEh2CiFEMIsvN9A9n25ySbNOV0x?usp=sharing**

**Documentation : https://drive.google.com/file/d/12g9LBZVk-e7lnpp9OQ-__drU9jsrusny/view?usp=sharing**





